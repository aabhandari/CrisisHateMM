{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlVCJp6C36-R"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALffo9kI228k"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset,DatasetDict\n",
        "from transformers import AutoTokenizer,TFAutoModelForSequenceClassification\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbNmwXry27OI"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"/content/LTI_Dataset.csv\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7buMk_583GNv"
      },
      "outputs": [],
      "source": [
        "pandas_df = pd.read_csv(DATA_PATH)\n",
        "pandas_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pandas_df = pandas_df.iloc[2058:]\n",
        "pandas_df = pandas_df.reset_index()"
      ],
      "metadata": {
        "id": "zg5ic0aUnCGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(pandas_df.shape[0]):\n",
        "  if(pandas_df['intent'][i]==2):\n",
        "    pandas_df['intent'][i]=0"
      ],
      "metadata": {
        "id": "_PpXoeozBjKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_df"
      ],
      "metadata": {
        "id": "6-dHqgOWzDK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_df = pandas_df[pandas_df['intent'].notna()]"
      ],
      "metadata": {
        "id": "xhQFdXr4YkjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_df['intent'] = pd.to_numeric(pandas_df['intent'],downcast='integer')"
      ],
      "metadata": {
        "id": "xRUOZ3-5ZIdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for index in pandas_df.index:\n",
        "#   if pandas_df['target'][index]==4:\n",
        "#     pandas_df.drop(index)\n",
        "# pandas_df = pandas_df.reset_index()\n",
        "\n",
        "# pandas_df = pandas_df[pandas_df.target != 4]"
      ],
      "metadata": {
        "id": "jc8S3S_OaWCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_df['intent'].value_counts()"
      ],
      "metadata": {
        "id": "CgmU8YpUzahr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y_Yy1EKOFibG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GHxNx14IFifC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYI-yF-IRyR6"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "Tweet = []\n",
        "for tweettext in pandas_df[\"text\"]:\n",
        "  text = re.sub(r\"http\\S+\", \"\", tweettext)\n",
        "  text = re.sub(r\"@\\S+\",\"\",text)\n",
        "  text = re.sub(r\"@\\S+\",\"\",text)\n",
        "  text = re.sub(r\".com$\",\"\",text)\n",
        "  text = re.sub(r\"@\",\"\",text)\n",
        "\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "  text = emoji_pattern.sub(r'',text)\n",
        "  text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
        "  #print(text)\n",
        "  Tweet.append(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRoLfdqB3SJ_"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "words = set(nltk.corpus.words.words())\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "wn=nltk.corpus.wordnet\n",
        "wn_lemmas = set(wn.all_lemma_names())\n",
        "len(wn_lemmas)\n",
        "words.update(['vladimir', 'putin', 'zelenskyy', 'zelensky', 'russia', 'ukraine', 'trump', 'biden', 'joe', 'US', 'usa', 'nukes', 'kyiv', 'kiev'])\n",
        "words.update(wn_lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_df[\"tweet_cleaned\"] = pd.Series(Tweet)\n",
        "\n",
        "text_cleaned = pandas_df[\"tweet_cleaned\"] \n",
        "texts_new = []\n",
        "\n",
        "for sentence in text_cleaned:\n",
        "    text_new = \" \".join(w.lower() for w in nltk.wordpunct_tokenize(sentence) if w.lower() in words or not w.isalpha()) \n",
        "    text_new = text_new.encode('ascii',errors='ignore').decode('ascii')\n",
        "    texts_new.append(text_new)\n",
        "\n",
        "pandas_df[\"tweet_cleaned\"] = pd.Series(texts_new)\n",
        "pandas_df"
      ],
      "metadata": {
        "id": "lHPl121mhpEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.summarization import summarize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "for index in pandas_df.index:\n",
        "  if len(pandas_df['tweet_cleaned'][index]) > 512:\n",
        "    text = pandas_df['tweet_cleaned'][index]\n",
        "    \n",
        "    try:\n",
        "      text = summarize(text, word_count = 50)\n",
        "    except:\n",
        "      list1 = list(text)\n",
        "      list1 = list1[:512]\n",
        "      text = ''.join(list1)\n",
        "\n",
        "    pandas_df['tweet_cleaned'][index] = text\n",
        "\n",
        "pandas_df = pandas_df.reset_index()\n",
        "pandas_df"
      ],
      "metadata": {
        "id": "1q5ONV2BhtIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_len = []\n",
        "for i in range(len(pandas_df['tweet_cleaned'])):\n",
        "  s = pandas_df['tweet_cleaned'][i].split() \n",
        "  words_len.append(len(s))"
      ],
      "metadata": {
        "id": "GqV7G-VIe4pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99jHOE-J3VcO"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "ds = Dataset.from_pandas(pandas_df)\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsF_CXtu3XfS"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset('csv', data_files=DATA_PATH, split='train')\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-McibgE3iAv"
      },
      "outputs": [],
      "source": [
        "train_test_valid = ds.train_test_split(test_size =0.20)\n",
        "\n",
        "test_valid = train_test_valid['test'].train_test_split(test_size=0.50)\n",
        "\n",
        "train_test_valid_dataset = DatasetDict({\n",
        "    'train': train_test_valid['train'],\n",
        "    'test': test_valid['test'],\n",
        "    'valid': test_valid['train']\n",
        "    })\n",
        "\n",
        "\n",
        "dataset = train_test_valid_dataset.remove_columns(['level_0','text', 'label', 'target','filename','index'])\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0LvDojW4Z6u"
      },
      "outputs": [],
      "source": [
        "model_type =  \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_type, use_fast=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDoTA1Js4ll5"
      },
      "outputs": [],
      "source": [
        "print(f\"Vocab size is : {tokenizer.vocab_size}\")\n",
        "\n",
        "print(f\"Model max length is : {tokenizer.model_max_length}\")\n",
        "\n",
        "print(f\"Model input names are: {tokenizer.model_input_names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsDONhQG4oqB"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(train_dataset):\n",
        "    return tokenizer(train_dataset['tweet_cleaned'], padding='max_length', truncation=True) \n",
        "\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "tokenized_dataset\n",
        "\n",
        "train_dataset = tokenized_dataset['train']\n",
        "eval_dataset = tokenized_dataset['valid']\n",
        "test_dataset = tokenized_dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PQOQNe_4qfu"
      },
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHqVjf2U4t0d"
      },
      "outputs": [],
      "source": [
        "# train_set = train_dataset.remove_columns([\"tweet_cleaned\",'level_0']).with_format('tensorflow')\n",
        "\n",
        "# tf_eval_dataset = eval_dataset.remove_columns([\"tweet_cleaned\",'level_0']).with_format('tensorflow')\n",
        "\n",
        "# tf_test_dataset = test_dataset.remove_columns([\"tweet_cleaned\",'level_0']).with_format('tensorflow')\n",
        "\n",
        "train_set = train_dataset.remove_columns([\"tweet_cleaned\"]).with_format('tensorflow')\n",
        "\n",
        "tf_eval_dataset = eval_dataset.remove_columns([\"tweet_cleaned\"]).with_format('tensorflow')\n",
        "\n",
        "tf_test_dataset = test_dataset.remove_columns([\"tweet_cleaned\"]).with_format('tensorflow')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dltR2D2Aow1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set"
      ],
      "metadata": {
        "id": "ixK1DyuegUKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6b8TETD4wT-"
      },
      "outputs": [],
      "source": [
        "train_features = { x: train_set[x] for x in tokenizer.model_input_names  }\n",
        "\n",
        "train_set_for_final_model = tf.data.Dataset.from_tensor_slices((train_features, train_set['intent'] ))\n",
        "\n",
        "train_set_for_final_model = train_set_for_final_model.shuffle(len(train_set)).batch(8)\n",
        "\n",
        "\n",
        "eval_features = {x: tf_eval_dataset[x] for x in tokenizer.model_input_names}\n",
        "val_set_for_final_model = tf.data.Dataset.from_tensor_slices((eval_features, tf_eval_dataset[\"intent\"]))\n",
        "val_set_for_final_model = val_set_for_final_model.batch(8)\n",
        "\n",
        "test_features = {x: tf_test_dataset[x] for x in tokenizer.model_input_names}\n",
        "test_set_for_final_model = tf.data.Dataset.from_tensor_slices((test_features, tf_test_dataset[\"intent\"]))\n",
        "test_set_for_final_model =test_set_for_final_model.batch(8)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_for_final_model"
      ],
      "metadata": {
        "id": "0bLKIObjgn8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3o3bl_yJ9aQ"
      },
      "outputs": [],
      "source": [
        "pandas_df[\"tweet_cleaned\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFdO9jS5I89c"
      },
      "outputs": [],
      "source": [
        "pip install livelossplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIGley3LJBPB"
      },
      "outputs": [],
      "source": [
        "from livelossplot import PlotLossesKeras\n",
        "from tensorflow.keras.layers import add, LSTM, Embedding, Dense\n",
        "callbacks = [PlotLossesKeras()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXDFI2Mh4yXV"
      },
      "outputs": [],
      "source": [
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_type,num_labels=2)\n",
        "# model = TFAutoModelForSequenceClassification.from_pretrained(\"/mnt/e0ccdbdb-22c3-4d9b-9413-fd976a2e99ae/M1/Code_Org/HF_Models/bert-base-uncased\", num_labels=3)\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "031rgLoY40K5"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_set_for_final_model, validation_data=val_set_for_final_model, epochs=3,callbacks=[callbacks],verbose=1 )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9pW1WClMmH0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeWkg0ms43bu"
      },
      "outputs": [],
      "source": [
        "preds = model.predict(test_set_for_final_model,verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqzgX8gR43Zd"
      },
      "outputs": [],
      "source": [
        "# model.save('/content/gdrive/MyDrive/WAR misinformation/bert_targets')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "id": "44rwDCg2oR9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seqeval\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report"
      ],
      "metadata": {
        "id": "mEQGdhhInQxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = []\n",
        "for i in range(len(test_valid['test']['intent'])):\n",
        "  l = test_valid['test']['intent'][i]\n",
        "  if(l==1):\n",
        "    test_labels.append(\"DIRECTED\")\n",
        "  elif(l==0):\n",
        "    test_labels.append(\"UNDIRECTED\")\n",
        "  "
      ],
      "metadata": {
        "id": "6sOxAOcRqIiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_labels = []\n",
        "for i in range(len(preds['logits'])):\n",
        "  p = np.argmax(preds['logits'][i])\n",
        "  if(p==1):\n",
        "    preds_labels.append(\"DIRECTED\")\n",
        "  elif(p==0):\n",
        "    preds_labels.append(\"UNDIRECTED\")\n"
      ],
      "metadata": {
        "id": "Z_KPD-Ruoit4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extractDigits(lst):\n",
        "    return [[el] for el in lst]"
      ],
      "metadata": {
        "id": "pTOSM_gNxX2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_labels=extractDigits(preds_labels)\n",
        "test_labels = extractDigits(test_labels)"
      ],
      "metadata": {
        "id": "HSMYl5uuxZRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(preds_labels).shape"
      ],
      "metadata": {
        "id": "eXyNr-y-qsL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(test_labels).shape"
      ],
      "metadata": {
        "id": "ZQnXP_Nrrrbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Dsez9pA43XA"
      },
      "outputs": [],
      "source": [
        "print(classification_report(preds_labels,test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1wz2KEU43Ue"
      },
      "outputs": [],
      "source": [
        "print(f1_score(test_labels, preds_labels,average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(precision_score(test_labels, preds_labels,average='macro'))"
      ],
      "metadata": {
        "id": "DXM8w31osg7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(recall_score(test_labels, preds_labels,average='macro'))"
      ],
      "metadata": {
        "id": "AmJokdHuybs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_test = []\n",
        "\n",
        "for i in test_labels:\n",
        "\n",
        "  if i == ['DIRECTED']:\n",
        "    pred=1\n",
        "  else:\n",
        "    pred=0\n",
        "  new_test.append(pred)\n",
        "\n",
        "new_pred = []\n",
        "for i in preds_labels:\n",
        "  if i == ['DIRECTED']:\n",
        "    pred=1\n",
        "  else:\n",
        "    pred=0\n",
        "  new_pred.append(pred)"
      ],
      "metadata": {
        "id": "pITpmeOgiCnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.metrics import macro_averaged_mean_absolute_error \n",
        "macro_averaged_mean_absolute_error(new_test, new_pred)"
      ],
      "metadata": {
        "id": "T_xGBR4kyg0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(new_test,new_pred)"
      ],
      "metadata": {
        "id": "SyMfW79NjhqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SIS7G7B23kQb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}