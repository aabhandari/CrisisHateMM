{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTgbSjCy2-kY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlVCJp6C36-R"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALffo9kI228k"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset,DatasetDict\n",
        "from transformers import AutoTokenizer,TFAutoModelForSequenceClassification\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbNmwXry27OI"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"/content/gdrive/MyDrive/LTI_Dataset.csv\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7buMk_583GNv"
      },
      "outputs": [],
      "source": [
        "pandas_df = pd.read_csv(DATA_PATH)\n",
        "pandas_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYI-yF-IRyR6"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "Tweet = []\n",
        "for tweettext in pandas_df[\"text\"]:\n",
        "  text = re.sub(r\"http\\S+\", \"\", tweettext)\n",
        "  text = re.sub(r\"@\\S+\",\"\",text)\n",
        "  text = re.sub(r\"@\\S+\",\"\",text)\n",
        "  text = re.sub(r\".com$\",\"\",text)\n",
        "  text = re.sub(r\"@\",\"\",text)\n",
        "\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "  text = emoji_pattern.sub(r'',text)\n",
        "  text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
        "  #print(text)\n",
        "  Tweet.append(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3sx5ImOwm_L"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "words = set(nltk.corpus.words.words())\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "wn=nltk.corpus.wordnet\n",
        "wn_lemmas = set(wn.all_lemma_names())\n",
        "len(wn_lemmas)\n",
        "words.update(['vladimir', 'putin', 'zelenskyy', 'zelensky', 'russia', 'ukraine', 'trump', 'biden', 'joe', 'US', 'usa', 'nukes', 'kyiv', 'kiev', 'republican', ''])\n",
        "words.update(wn_lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRoLfdqB3SJ_"
      },
      "outputs": [],
      "source": [
        "pandas_df[\"tweet_cleaned\"] = pd.Series(Tweet)\n",
        "\n",
        "text_cleaned = pandas_df[\"tweet_cleaned\"] \n",
        "texts_new = []\n",
        "\n",
        "for sentence in text_cleaned:\n",
        "    text_new = \" \".join(w for w in nltk.wordpunct_tokenize(sentence) if w.lower() in words or not w.isalpha()) \n",
        "    text_new = text_new.encode('ascii',errors='ignore').decode('ascii')\n",
        "    texts_new.append(text_new)\n",
        "\n",
        "pandas_df[\"tweet_cleaned\"] = pd.Series(texts_new)\n",
        "pandas_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIHhgrvJf6t3"
      },
      "outputs": [],
      "source": [
        "# from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n",
        "# tokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "# model=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "import gensim\n",
        "from gensim.summarization import summarize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "for index in pandas_df.index:\n",
        "  if len(pandas_df['tweet_cleaned'][index]) > 512:\n",
        "    text = pandas_df['tweet_cleaned'][index]\n",
        "    \n",
        "    try:\n",
        "      text = summarize(text, word_count = 50)\n",
        "    except:\n",
        "      list1 = list(text)\n",
        "      list1 = list1[:512]\n",
        "      text = ''.join(list1)\n",
        "\n",
        "    pandas_df['tweet_cleaned'][index] = text\n",
        "       # inputs = tokenizer.batch_encode_plus([pandas_df['tweet_cleaned'][index]],return_tensors='pt')\n",
        "    # summary_ids = model.generate(inputs['input_ids'], early_stopping=True)\n",
        "    # bart_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    # pandas_df['tweet_cleaned'][index] = bart_summary\n",
        "\n",
        "pandas_df = pandas_df.reset_index()\n",
        "pandas_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WG5Szjw4Rjc"
      },
      "outputs": [],
      "source": [
        "words_len = []\n",
        "for i in range(len(pandas_df['tweet_cleaned'])):\n",
        "  s = pandas_df['tweet_cleaned'][i].split() \n",
        "  words_len.append(len(s))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3nPIniK51WK"
      },
      "outputs": [],
      "source": [
        "largest_sen = max(wordlen for wordlen in words_len)\n",
        "print('biggest sentence has {} words'.format(largest_sen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muzwo0XP8FCY"
      },
      "outputs": [],
      "source": [
        "largest_sen = max(len(tweettext) for tweettext in pandas_df['tweet_cleaned'])\n",
        "print('biggest sentence has {} characters'.format(largest_sen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99jHOE-J3VcO"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "ds = Dataset.from_pandas(pandas_df)\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsF_CXtu3XfS"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset('csv', data_files=DATA_PATH, split='train')\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-McibgE3iAv"
      },
      "outputs": [],
      "source": [
        "train_test_valid = ds.train_test_split(test_size =0.3)\n",
        "\n",
        "test_valid = train_test_valid['test'].train_test_split(test_size=0.5)\n",
        "\n",
        "train_test_valid_dataset = DatasetDict({\n",
        "    'train': train_test_valid['train'],\n",
        "    'test': test_valid['test'],\n",
        "    'valid': test_valid['train']\n",
        "    })\n",
        "\n",
        "\n",
        "dataset = train_test_valid_dataset.remove_columns(['text', 'target', 'intent','filename','index'])\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0LvDojW4Z6u"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDoTA1Js4ll5"
      },
      "outputs": [],
      "source": [
        "print(f\"Vocab size is : {tokenizer.vocab_size}\")\n",
        "\n",
        "print(f\"Model max length is : {tokenizer.model_max_length}\")\n",
        "\n",
        "print(f\"Model input names are: {tokenizer.model_input_names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsDONhQG4oqB"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(train_dataset):\n",
        "    return tokenizer(train_dataset['tweet_cleaned'], padding='max_length', truncation=True) \n",
        "\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "tokenized_dataset\n",
        "\n",
        "train_dataset = tokenized_dataset['train']\n",
        "eval_dataset = tokenized_dataset['valid']\n",
        "test_dataset = tokenized_dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PQOQNe_4qfu"
      },
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHqVjf2U4t0d"
      },
      "outputs": [],
      "source": [
        "train_set = train_dataset.remove_columns([\"tweet_cleaned\"]).with_format('tensorflow')\n",
        "\n",
        "tf_eval_dataset = eval_dataset.remove_columns([\"tweet_cleaned\"]).with_format('tensorflow')\n",
        "\n",
        "tf_test_dataset = test_dataset.remove_columns([\"tweet_cleaned\"]).with_format('tensorflow')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6b8TETD4wT-"
      },
      "outputs": [],
      "source": [
        "train_features = { x: train_set[x] for x in tokenizer.model_input_names  }\n",
        "\n",
        "train_set_for_final_model = tf.data.Dataset.from_tensor_slices((train_features, train_set['label'] ))\n",
        "\n",
        "train_set_for_final_model = train_set_for_final_model.shuffle(len(train_set)).batch(8)\n",
        "\n",
        "\n",
        "eval_features = {x: tf_eval_dataset[x] for x in tokenizer.model_input_names}\n",
        "val_set_for_final_model = tf.data.Dataset.from_tensor_slices((eval_features, tf_eval_dataset[\"label\"]))\n",
        "val_set_for_final_model = val_set_for_final_model.batch(8)\n",
        "\n",
        "test_features = {x: tf_test_dataset[x] for x in tokenizer.model_input_names}\n",
        "test_set_for_final_model = tf.data.Dataset.from_tensor_slices((test_features, tf_test_dataset[\"label\"]))\n",
        "test_set_for_final_model =test_set_for_final_model.batch(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3o3bl_yJ9aQ"
      },
      "outputs": [],
      "source": [
        "pandas_df[\"tweet_cleaned\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFdO9jS5I89c"
      },
      "outputs": [],
      "source": [
        "pip install livelossplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIGley3LJBPB"
      },
      "outputs": [],
      "source": [
        "from livelossplot import PlotLossesKeras\n",
        "callbacks = [PlotLossesKeras()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXDFI2Mh4yXV"
      },
      "outputs": [],
      "source": [
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "# model = TFAutoModelForSequenceClassification.from_pretrained(\"/mnt/e0ccdbdb-22c3-4d9b-9413-fd976a2e99ae/M1/Code_Org/HF_Models/bert-base-uncased\", num_labels=3)\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sQzpgo8e40x"
      },
      "outputs": [],
      "source": [
        "\n",
        "# tf.keras.backend.set_floatx('float16')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "031rgLoY40K5"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_set_for_final_model, validation_data=val_set_for_final_model, epochs=5,callbacks=[callbacks],verbose=1 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYLS7d4i43d-"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_set_for_final_model,verbose=1)\n",
        "print('\\nTest accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeWkg0ms43bu"
      },
      "outputs": [],
      "source": [
        "preds = model.predict(test_set_for_final_model,verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqzgX8gR43Zd"
      },
      "outputs": [],
      "source": [
        "# model.save('/content/gdrive/MyDrive/WAR misinformation/bert_labels')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44rwDCg2oR9k"
      },
      "outputs": [],
      "source": [
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEQGdhhInQxJ"
      },
      "outputs": [],
      "source": [
        "import seqeval\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sOxAOcRqIiV"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_labels = []\n",
        "for i in range(len(test_valid['test']['label'])):\n",
        "  l = test_valid['test']['label'][i]\n",
        "  if(l==1):\n",
        "    test_labels.append(\"HATE\")\n",
        "  else:\n",
        "    test_labels.append(\"NON-HATE\")\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_KPD-Ruoit4"
      },
      "outputs": [],
      "source": [
        "preds_labels = []\n",
        "for i in range(len(preds['logits'])):\n",
        "  p = np.argmax(preds['logits'][i])\n",
        "  if(p==1):\n",
        "    preds_labels.append(\"HATE\")\n",
        "  else:\n",
        "    preds_labels.append(\"NON-HATE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTOSM_gNxX2A"
      },
      "outputs": [],
      "source": [
        "def doublylist(lst):\n",
        "    return [[w] for w in lst]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSMYl5uuxZRh"
      },
      "outputs": [],
      "source": [
        "preds_labels = doublylist(preds_labels)\n",
        "test_labels = doublylist(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXyNr-y-qsL2"
      },
      "outputs": [],
      "source": [
        "np.array(preds_labels).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQnXP_Nrrrbw"
      },
      "outputs": [],
      "source": [
        "np.array(test_labels).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Dsez9pA43XA"
      },
      "outputs": [],
      "source": [
        "print(classification_report(test_labels, preds_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1wz2KEU43Ue"
      },
      "outputs": [],
      "source": [
        "print(f1_score(test_labels, preds_labels,average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXM8w31osg7A"
      },
      "outputs": [],
      "source": [
        "print(precision_score(test_labels, preds_labels,average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmJokdHuybs-"
      },
      "outputs": [],
      "source": [
        "print(recall_score(test_labels, preds_labels, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b214TXIfeuXB"
      },
      "outputs": [],
      "source": [
        "new_test = []\n",
        "for i in test_labels:\n",
        "  if i == ['HATE']:\n",
        "    pred=1\n",
        "  else:\n",
        "    pred=0\n",
        "  new_test.append(pred)\n",
        "\n",
        "new_pred = []\n",
        "\n",
        "for i in preds_labels:\n",
        "  if i ==['HATE']:\n",
        "    pred=1\n",
        "  else:\n",
        "    pred=0\n",
        "  new_pred.append(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_xGBR4kyg0J"
      },
      "outputs": [],
      "source": [
        "from imblearn.metrics import macro_averaged_mean_absolute_error \n",
        "macro_averaged_mean_absolute_error(new_test, new_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WREpJlgd5gcc"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04BqCbS4jLiT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}