{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kGdytEL-cqR"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "A40qSNh1Cye3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thSYatdRBiP6"
      },
      "outputs": [],
      "source": [
        "# !unzip \"/content/drive/MyDrive/Dataset.zip\" -d \"/content/drive/MyDrive/CrisisMMImages\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62eDxz05BzPC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from time import sleep\n",
        "import warnings\n",
        "from transformers import AdamW\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AunCHFubBxUO"
      },
      "outputs": [],
      "source": [
        "# read annotations file\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Text Benchmarking/LTI_Dataset.csv\")\n",
        "\n",
        "\n",
        "data = data.iloc[2058:]\n",
        "data = data.reset_index()\n",
        "\n",
        "for i in range(data.shape[0]):\n",
        "  if(data['intent'][i]==2):\n",
        "    data['intent'][i]=0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['intent'].value_counts()"
      ],
      "metadata": {
        "id": "vL3swv7b2RaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "Tweet = []\n",
        "for tweettext in data[\"text\"]:\n",
        "  text = re.sub(r\"http\\S+\", \"\", tweettext)\n",
        "  text = re.sub(r\"@\\S+\",\"\",text)\n",
        "  text = re.sub(r\"@\\S+\",\"\",text)\n",
        "  text = re.sub(r\".com$\",\"\",text)\n",
        "  text = re.sub(r\"@\",\"\",text)\n",
        "\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "  text = emoji_pattern.sub(r'',text)\n",
        "  text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
        "  #print(text)\n",
        "  Tweet.append(text)"
      ],
      "metadata": {
        "id": "GBZbEwtXM7pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "words = set(nltk.corpus.words.words())\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "wn=nltk.corpus.wordnet\n",
        "wn_lemmas = set(wn.all_lemma_names())\n",
        "len(wn_lemmas)\n",
        "words.update(['vladimir', 'putin', 'zelenskyy', 'zelensky', 'russia', 'ukraine', 'trump', 'biden', 'joe', 'US', 'usa', 'nukes', 'kyiv', 'kiev'])\n",
        "words.update(wn_lemmas)"
      ],
      "metadata": {
        "id": "kT4rWDBaGzMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"tweet_cleaned\"] = pd.Series(Tweet)\n",
        "\n",
        "text_cleaned = data[\"tweet_cleaned\"] \n",
        "texts_new = []\n",
        "\n",
        "for sentence in text_cleaned:\n",
        "    text_new = \" \".join(w.lower() for w in nltk.wordpunct_tokenize(sentence) if w.lower() in words or not w.isalpha()) \n",
        "    text_new = text_new.encode('ascii',errors='ignore').decode('ascii')\n",
        "    texts_new.append(text_new)\n",
        "\n",
        "data[\"tweet_cleaned\"] = pd.Series(texts_new)\n",
        "data"
      ],
      "metadata": {
        "id": "P-ajtYDkNKKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n",
        "# tokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "# model=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "import gensim\n",
        "from gensim.summarization import summarize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "for index in data.index:\n",
        "  if len(data['tweet_cleaned'][index]) > 77:\n",
        "    text = data['tweet_cleaned'][index]\n",
        "    \n",
        "    try:\n",
        "      text = summarize(text, word_count = 50)\n",
        "    except:\n",
        "      list1 = list(text)\n",
        "      list1 = list1[:77]\n",
        "      text = ''.join(list1)\n",
        "\n",
        "    data['tweet_cleaned'][index] = text\n",
        "       # inputs = tokenizer.batch_encode_plus([pandas_df['tweet_cleaned'][index]],return_tensors='pt')\n",
        "    # summary_ids = model.generate(inputs['input_ids'], early_stopping=True)\n",
        "    # bart_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    # pandas_df['tweet_cleaned'][index] = bart_summary\n",
        "\n",
        "data = data.reset_index()\n",
        "data"
      ],
      "metadata": {
        "id": "72tvwpAPNNKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oaj5hdsECHEw"
      },
      "outputs": [],
      "source": [
        "ids = (data['filename'].values)\n",
        "\n",
        "texts = (data['tweet_cleaned'].values)\n",
        "\n",
        "labels = (data['intent'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc6YxJM_CZEu"
      },
      "outputs": [],
      "source": [
        "# DATASET\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    This is our custom dataset class which will load the text and their corresponding labels into Pytorch tensors\n",
        "    \"\"\"\n",
        "    def __init__(self, labels, text, ids):\n",
        "        self.labels = labels\n",
        "        self.texts = text\n",
        "        self.ids = ids\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {}\n",
        "        id = self.ids[idx]\n",
        "\n",
        "        text = self.texts[idx]\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        image_name = str(id) \n",
        "\n",
        "        img_path = os.path.join('/content/drive/MyDrive/CrisisMMImages/Dataset', image_name)\n",
        "\n",
        "        try:\n",
        "            sample[\"label\"] = int(label)\n",
        "            sample[\"text\"] = text\n",
        "            sample[\"image\"] = img_path\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "        \n",
        "        return sample\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset(labels, texts, ids)"
      ],
      "metadata": {
        "id": "SBYO97KcO_lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "GXDZfUai2ftj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split into training, validation and testing\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [2265, 400])\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [1865, 400])"
      ],
      "metadata": {
        "id": "QrzyPhENPEr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Specify the Hyper parameters \n",
        "\n",
        "BATCH_SIZE = 4\n",
        "NUM_LABELS = 2   # since we have two labels - secuure and insecure i.e 0 and 1\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "# Check GPU\n",
        "device = 'cpu'\n",
        "print(device)\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.enabled   = True"
      ],
      "metadata": {
        "id": "iWnBZGueTKgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## We call the dataloader class\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    pin_memory=True,\n",
        "    num_workers=0,\n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        " )\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    pin_memory=True,\n",
        "    num_workers=0,\n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        " )\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    pin_memory=True,\n",
        "    num_workers=0,\n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        " )\n",
        "\n",
        "dataloaders = {'Train': train_loader, 'Test': test_loader, 'Val': val_loader}\n",
        "\n"
      ],
      "metadata": {
        "id": "00ecG1syTHWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#attach a classification layer\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(1280, 128)\n",
        "        self.fc2 = nn.Linear(128, 2)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "      linear_out = self.dropout(F.relu(self.fc1(input)))    \n",
        "\n",
        "      final_out = self.fc2(linear_out)    \n",
        "\n",
        "      return final_out"
      ],
      "metadata": {
        "id": "ucRkyAVaTO2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "model = Model().to(device)"
      ],
      "metadata": {
        "id": "iRC2KlDeeFGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = LEARNING_RATE, eps=1e-8)\n",
        "\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "#Loss function\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "LGQ7cWVqeaBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "\n",
        "for epoch in range(0, EPOCHS):\n",
        "  print('-'*50)\n",
        "  print('Epoch {}/{}'.format(epoch+1, EPOCHS))\n",
        "  \n",
        "  for phase in ['Train', 'Val']:\n",
        "\n",
        "    batch_loss = 0.0000   #live loss\n",
        "    batch_acc = 0.0000   #live accuracy\n",
        "    \n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    if phase == 'Train':\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    \n",
        "    with tqdm(dataloaders[phase], unit=\"batch\", desc=phase) as tepoch:\n",
        "\n",
        "      for idx, batch in enumerate(tepoch):\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        text = batch[\"text\"]\n",
        "\n",
        "        imgs = []\n",
        "\n",
        "        img_paths = batch[\"image\"]\n",
        "\n",
        "        for path in img_paths:\n",
        "\n",
        "          imgs.append((Image.open(path)))\n",
        "\n",
        "        \n",
        "        inputs = processor(text=text, images=imgs, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        clip_output = clip(**inputs)\n",
        "\n",
        "        img_embed = clip_output.vision_model_output.pooler_output\n",
        "\n",
        "        text_embed = clip_output.text_model_output.pooler_output\n",
        "\n",
        "        concat = torch.cat((text_embed, img_embed), 1)\n",
        "\n",
        "        output = model(concat)\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        if phase == 'Train':\n",
        "\n",
        "            #zero gradients\n",
        "            optimizer.zero_grad() \n",
        "\n",
        "            # Backward pass  (calculates the gradients)\n",
        "            loss.backward()     \n",
        "\n",
        "            optimizer.step()             # Updates the weights    \n",
        "\n",
        "        batch_loss += loss.item()\n",
        "\n",
        "        _, preds = output.data.max(1)\n",
        "\n",
        "        y_pred.extend(preds.tolist())\n",
        "        y_true.extend(labels.tolist())\n",
        "        \n",
        "        tepoch.set_postfix(loss = batch_loss/(idx+1))\n",
        "\n",
        "        #accumulate loss and accuracy\n",
        "\n",
        "      if phase == 'Train':\n",
        "        train_loss.append(batch_loss/(idx+1))\n",
        "        train_acc.append(batch_acc)\n",
        "\n",
        "      else:\n",
        "        val_loss.append(batch_loss/(idx+1))\n",
        "        val_acc.append(batch_acc)\n",
        "\n"
      ],
      "metadata": {
        "id": "erDQFfVOecZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "with tqdm(dataloaders['Test'], unit=\"batch\", desc=phase) as tepoch:\n",
        "  for idx, batch in enumerate(tepoch):\n",
        "\n",
        "    labels = batch[\"label\"].to(device)\n",
        "    text = batch[\"text\"]\n",
        "\n",
        "    imgs = []\n",
        "\n",
        "    img_paths = batch[\"image\"]\n",
        "\n",
        "    for path in img_paths:\n",
        "\n",
        "      imgs.append((Image.open(path)))\n",
        "    \n",
        "    inputs = processor(text=text, images=imgs, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    clip_output = clip(**inputs)\n",
        "\n",
        "    img_embed = clip_output.vision_model_output.pooler_output\n",
        "\n",
        "    text_embed = clip_output.text_model_output.pooler_output\n",
        "\n",
        "    concat = torch.cat((text_embed, img_embed), 1)\n",
        "\n",
        "    output = model(concat)\n",
        "\n",
        "    _, preds = output.data.max(1)\n",
        "\n",
        "    y_pred.extend(preds.tolist())\n",
        "    y_true.extend(labels.tolist())\n",
        "\n"
      ],
      "metadata": {
        "id": "bEqqdS4qNpOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, accuracy_score"
      ],
      "metadata": {
        "id": "t1XJ-0J0wXwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_true,y_pred)"
      ],
      "metadata": {
        "id": "o1qE9V8MxAKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_true, y_pred, average='macro')"
      ],
      "metadata": {
        "id": "dFqhYozMwfrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_score(y_true, y_pred, average='macro')"
      ],
      "metadata": {
        "id": "zKhy_AWLaFwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recall_score(y_true, y_pred, average='macro')"
      ],
      "metadata": {
        "id": "UwUR9c4DZGsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.metrics import macro_averaged_mean_absolute_error \n",
        "macro_averaged_mean_absolute_error(y_true, y_pred)"
      ],
      "metadata": {
        "id": "8l_0xiHaaXl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uGNwebNyaq-z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}